{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "6ca4ecdb689e464bb101d9105d782fc7ce302962d26c8c5b6f403fcd2bd8be3b"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Hands-on Lab: Considerations for Data Professionals using GenAI\n\n**Estimated time needed:** 45 minutes \n\n## Overview  \n\nIn this lab, you will assess and reinforce your understanding of key principles related to the ethical deployment of generative AI, specifically focusing on transparency, fairness, responsibility, accountability, and reliability.  \n\nYou will be presented with scenarios, and you are expected to provide a solution for the question based on the scenario. To help you with the solutions, a hint is provided for each exercise. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Learning Objectives \n\nAfter completing this lab, you will be able to: \n\n - Maintain transparency and fairness in your AI system \n\n - Ensure accountability in the deployment of AI chatbot \n\n - Enhance the reliability of your AI model to ensure accurate product descriptions \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 1:  \n\nYou are developing a generative AI system that creates personalized content recommendations for users. The system seems to consistently recommend content that aligns with certain cultural and demographic biases.  \n\nUsers from diverse backgrounds are expressing concern about the lack of transparency and fairness in the recommendations.  \n\nHow do you maintain transparency and fairness in your AI system? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 1. **Bias Identification and Mitigation**\n   - **Data Audit**: Perform a thorough audit of the data used to train the model to identify any cultural or demographic biases.\n   - **Diverse Training Data**: Ensure that your training data represents a wide range of cultural, demographic, and socioeconomic backgrounds to minimize bias.\n   - **Bias Detection Tools**: Utilize tools and techniques to detect and measure biases in the model's outputs, such as fairness metrics.\n\n### 2. **Algorithmic Transparency**\n   - **Explainability**: Develop and integrate mechanisms that allow users to understand how recommendations are generated. This might include providing explanations for why certain content is recommended.\n   - **User Controls**: Offer users the ability to adjust their preferences or provide feedback on the recommendations to influence the model's outputs. \n\n### 3. **Inclusive Design**\n   - **Diverse Teams**: Involve a diverse team of developers and stakeholders in the design and deployment process to ensure that multiple perspectives are considered.\n   - **User Testing**: Conduct user testing with a diverse group of participants to identify any fairness issues and make necessary adjustments before full deployment.\n\n### 4. **Transparent Communication**\n   - **Clear Disclosures**: Communicate to users how the recommendation system works, including any limitations or potential biases. This could be done through a user guide, FAQ, or in-app notifications.\n   - **Feedback Mechanism**: Implement a transparent feedback mechanism where users can report any perceived unfairness or bias, and ensure that this feedback is addressed promptly.\n\n### 5. **Ongoing Monitoring and Adjustment**\n   - **Continuous Monitoring**: Regularly monitor the system's outputs to detect and correct any biases that may emerge over time.\n   - **Model Updates**: Update the model periodically to improve fairness, incorporating new data and addressing any identified biases.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nConsider steps like conducting a bias assessment, enhancing diversity in training data, implementing explainability features, and establishing a user feedback loop to ensure fairness and transparency in your AI system. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nTo address this issue, you could implement the following steps: \n\n1. Conduct a thorough bias assessment to identify and understand the biases present in the training data and algorithms. \n2. Use specialized tools or metrics to measure and quantify biases in content recommendations. \n3. Enhance the diversity of your training data by including a broader range of cultural, demographic, and user behavior data. \n4. Ensure that the training data reflects the diversity of your user base to reduce biases. \n5. Implement explainability features to provide users with insights into why specific recommendations are made. \n6. Offer transparency by showing the key factors and attributes influencing the recommendations. \n7. Establish a user feedback loop where users can report biased recommendations or provide feedback on content relevance. \n8. Regularly analyze this feedback to iteratively improve the system's fairness. \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">**Additional Information**\n\n>Some specialized tools that can be used to measure and quantify biases: \n\n>Holistic AI Library: This open-source library offers a range of metrics and mitigation strategies for various AI tasks, including content recommendation. It analyzes data for bias across different dimensions and provides visualizations for clear understanding. \n\n>Fairness 360: IBM&#39;s Fairness 360 toolkit provides various tools like Aequitas and What-If Tool to analyze bias in data sets, models, and decision-making processes. It offers metrics like statistical parity, differential odds ratio, and counterfactual fairness. IBM moved AI Fairness 360 to LF AI in July 2020. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 2  \n\nYour company has deployed a chatbot powered by generative AI to interact with customers. The chatbot occasionally generates responses that are inappropriate or offensive, leading to customer dissatisfaction. As the AI developer, how do you take responsibility for these incidents and ensure accountability in the deployment of the AI chatbot? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 1. **Immediate Response and Accountability**\n   - **Acknowledge Issues**: Publicly acknowledge the issue and take responsibility for the chatbot's inappropriate responses. Communicate to customers that their concerns are taken seriously and that steps are being taken to address the problem.\n   - **Customer Support**: Provide immediate customer support to those affected, offering apologies and compensations where necessary. Ensure that customers can easily report offensive interactions.\n\n### 2. **Content Moderation and Filters**\n   - **Implement Safeguards**: Introduce content moderation filters to detect and block inappropriate or offensive language before it reaches customers. These filters should be regularly updated to catch new types of inappropriate content.\n   - **Human Oversight**: Establish human-in-the-loop mechanisms where human moderators can review certain types of responses generated by the chatbot before they are sent to customers.\n\n### 3. **Bias and Harm Reduction**\n   - **Bias Mitigation**: Utilize tools like the **Holistic AI Library** or **Fairness 360** toolkit to assess and mitigate biases that might contribute to inappropriate responses. These tools can help identify and remove harmful biases in the training data or model outputs.\n   - **Training on Ethical Content**: Re-train the model using data that emphasizes ethical and non-offensive content. Incorporate diverse datasets to minimize the likelihood of generating biased or offensive content.\n\n### 4. **Transparent Reporting and Communication**\n   - **Incident Reporting**: Create a transparent incident reporting system where all instances of inappropriate responses are logged, reviewed, and addressed. Regularly publish reports on how these incidents are being managed and what steps are being taken to improve the system.\n   - **User Feedback Integration**: Establish a feedback loop where users can easily report issues directly from their chat interface. Use this feedback to continuously refine and improve the chatbot's performance.\n\n### 5. **Ongoing Monitoring and Updates**\n   - **Real-Time Monitoring**: Implement real-time monitoring tools to detect when the chatbot generates potentially harmful or offensive content. Set up alerts to notify developers or moderators immediately when such incidents occur.\n   - **Regular Updates**: Regularly update the chatbot’s algorithms and data sources to ensure that it evolves to handle new language, context, and ethical considerations. This includes updating training data to reflect new societal norms and language usage.\n\n### 6. **Ethical Guidelines and Governance**\n   - **Ethical Guidelines**: Develop and enforce clear ethical guidelines for the design and deployment of the AI chatbot. Ensure that these guidelines are aligned with the company's values and customer expectations.\n   - **AI Governance Framework**: Establish an AI governance framework that defines roles, responsibilities, and procedures for handling ethical issues related to AI. This includes setting up an ethics committee to oversee AI deployments and handle any ethical dilemmas that arise.\n\n### 7. **Legal and Regulatory Compliance**\n   - **Compliance Checks**: Ensure that the chatbot complies with all relevant legal and regulatory requirements, especially those related to data protection, discrimination, and consumer rights. Regularly review these requirements as they evolve.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nTo address responsibility and accountability, analyze errors, respond swiftly, continuously monitor for inappropriate responses, and communicate openly with stakeholders about corrective actions taken to improve the chatbot&#39;s behavior. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nAddressing responsibility and accountability in this scenario involves the following steps: \n\n1. Conduct a detailed analysis of the inappropriate responses to identify patterns and root causes. \n2. Determine whether the issues stem from biased training data, algorithmic limitations, or other factors. \n3. Implement a mechanism to quickly identify and rectify inappropriate responses by updating the chatbot&#39;s training data or fine-tuning the model. \n4. Communicate openly with affected customers, acknowledge the issue, and assure them of prompt corrective actions. \n5. Set up continuous monitoring systems to detect and flag inappropriate responses in real-time. \n6. Implement alerts or human-in-the-loop mechanisms to intervene when the system generates potentially harmful content. \n7. Clearly communicate the steps taken to address the issue to both internal stakeholders and customers. \n8. Emphasize the commitment to continuous improvement and the responsible use of AI technology. \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 3: \n\nYour company has developed a generative AI model that autonomously generates product descriptions for an e-commerce platform. However, users have reported instances where the generated descriptions contain inaccurate information, leading to customer confusion and dissatisfaction. How do you enhance the reliability of your AI model to ensure accurate product descriptions? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<i>Type your response here</i>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nTo improve reliability, focus on quality assurance testing, use domain-specific training data, adopt an iterative model training approach, and integrate user feedback to iteratively correct errors and enhance the accuracy of the AI-generated product descriptions. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nTo improve the reliability of the AI model in generating product descriptions, consider the following actions: \n\n1. Implement rigorous quality assurance testing to evaluate the accuracy of the generated product descriptions. \n2. Create a comprehensive testing data set that covers a wide range of products and scenarios to identify and address inaccuracies. \n3. Ensure that the AI model is trained on a diverse and extensive data set specific to the e-commerce domain. \n4. Include product information from reputable sources to enhance the model's understanding of accurate product details. \n5. Implement an iterative training approach to continuously update and improve the model based on user feedback and evolving product data. \n6. Regularly retrain the model to adapt to changes in the product catalog and user preferences. \n7. Encourage users to provide feedback on inaccurate product descriptions. \n8. Use this feedback to fine-tune the model, correct errors, and improve the overall reliability of the AI-generated content.  \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Congratulations! You have completed the lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Dr. Pooja](https://www.linkedin.com/in/p-b28802262/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Abhishek Gagneja](https://www.linkedin.com/in/abhishek-gagneja-23051987/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--## Change Log--!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2023-12-14|0.1|Abhishek Gagneja|Initial Draft created| --!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Copyright © 2023 IBM Corporation. All rights reserved.\n",
      "metadata": {}
    }
  ]
}